{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\anaconda3\\envs\\natural-language-processing\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler, AdamW, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict, load_metric\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jehoiada Wong\\AppData\\Local\\Temp\\ipykernel_2448\\1636561662.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('preprocessed_tweets2.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text2</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>contains_price</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-19</td>\n",
       "      <td>do you even mine #bitcoin dude â¦ do you even...</td>\n",
       "      <td>False</td>\n",
       "      <td>['do', 'you', 'even', 'mine', 'bitcoin', 'dude...</td>\n",
       "      <td>do you even mine bitcoin dude do you even run ...</td>\n",
       "      <td>['#bitcoin']</td>\n",
       "      <td>even mine bitcoin dude even run noteâ even un...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-18</td>\n",
       "      <td>#bitcoin is whales and big companys instrumen...</td>\n",
       "      <td>False</td>\n",
       "      <td>['bitcoin', 'is', 'whales', 'and', 'big', 'com...</td>\n",
       "      <td>bitcoin is whales and big companys instrument ...</td>\n",
       "      <td>['#bitcoin', '#btc']</td>\n",
       "      <td>bitcoin whales big companys instrument exchan...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>ð $hmc ð\\nhospitality monkey coin \\ncha...</td>\n",
       "      <td>False</td>\n",
       "      <td>['hmc', 'hospitality', 'monkey', 'coin', 'char...</td>\n",
       "      <td>hmc hospitality monkey coin charity oriented t...</td>\n",
       "      <td>['#bsc', '#btc', '#ethereum']</td>\n",
       "      <td>hmc hospitality monkey coin charity oriented ...</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               text is_retweet  \\\n",
       "0  2021-07-19  do you even mine #bitcoin dude â¦ do you even...      False   \n",
       "1  2021-07-18   #bitcoin is whales and big companys instrumen...      False   \n",
       "2  2021-08-08   ð $hmc ð\\nhospitality monkey coin \\ncha...      False   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  ['do', 'you', 'even', 'mine', 'bitcoin', 'dude...   \n",
       "1  ['bitcoin', 'is', 'whales', 'and', 'big', 'com...   \n",
       "2  ['hmc', 'hospitality', 'monkey', 'coin', 'char...   \n",
       "\n",
       "                                               text2  \\\n",
       "0  do you even mine bitcoin dude do you even run ...   \n",
       "1  bitcoin is whales and big companys instrument ...   \n",
       "2  hmc hospitality monkey coin charity oriented t...   \n",
       "\n",
       "                        hashtags  \\\n",
       "0                   ['#bitcoin']   \n",
       "1           ['#bitcoin', '#btc']   \n",
       "2  ['#bsc', '#btc', '#ethereum']   \n",
       "\n",
       "                              text_without_stopwords  hashtag_count  \\\n",
       "0   even mine bitcoin dude even run noteâ even un...              1   \n",
       "1   bitcoin whales big companys instrument exchan...              2   \n",
       "2   hmc hospitality monkey coin charity oriented ...              3   \n",
       "\n",
       "   tweet_length  contains_price  labels  \n",
       "0            15           False       0  \n",
       "1            26           False       1  \n",
       "2            25           False       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('preprocessed_tweets2.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f'siebert/sentiment-roberta-large-english'\n",
    "model = torch.load('RoBERTA-1-epoch.pt', map_location='cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the accuracy of the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a sample of 500 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   190,  4318,  ...,     1,     1,     1],\n",
      "        [    0, 11388, 18018,  ...,     1,     1,     1],\n",
      "        [    0,  1368, 29297,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,    82,   202,  ...,     1,     1,     1],\n",
      "        [    0, 11388,   614,  ...,     1,     1,     1],\n",
      "        [    0, 11388,  2935,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "Accuracy: 55.800000000000004%\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(list(data['text_without_stopwords'][:500]),\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  max_length=512,\n",
    "                  return_tensors='pt')\n",
    "print(batch)\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**batch)\n",
    "    preds = torch.softmax(outputs.logits, dim=1)\n",
    "    preds = torch.argmax(preds, dim=1)\n",
    "acc = accuracy_score(data['labels'][:500], preds)\n",
    "print(f'Accuracy: {acc*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not very great we will need to train it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['date', 'text', 'is_retweet', 'tokenized_text', 'text2', 'hashtags', 'text_without_stopwords', 'hashtag_count', 'tweet_length', 'contains_price', 'labels'],\n",
       "        num_rows: 80000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['date', 'text', 'is_retweet', 'tokenized_text', 'text2', 'hashtags', 'text_without_stopwords', 'hashtag_count', 'tweet_length', 'contains_price', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['date', 'text', 'is_retweet', 'tokenized_text', 'text2', 'hashtags', 'text_without_stopwords', 'hashtag_count', 'tweet_length', 'contains_price', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"csv\", data_files=\"preprocessed_tweets2.csv\")\n",
    "\n",
    "train_data = raw_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "test_valid = train_data['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1803, 26713, 821, 16100, 48726, 923, 5505, 3070, 39398, 7664, 11940, 32605, 2405, 11388, 326, 11726, 1638, 25616, 16776, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train']['text_without_stopwords'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the training dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.28 %\n"
     ]
    }
   ],
   "source": [
    "print(round(np.sum(dataset['train']['labels'])/len(dataset['train'])*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the train dataset consist of 47% of 1s which should be balanced enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text_without_stopwords'], truncation=True, max_length=512, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226c3145ec9246b99a25a436329f53f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 80000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['date', 'text', 'is_retweet', 'tokenized_text', 'text2', 'hashtags', 'text_without_stopwords', 'hashtag_count', 'tweet_length', 'contains_price'])\n",
    "tokenized_dataset.set_format('torch')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (80000, 3), 'test': (10000, 3), 'valid': (10000, 3)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the tokenized_dataset into a dataloader so we can batch it easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 1, 1, 0, 0, 0])\n",
      "{'input_ids': torch.Size([8, 84]), 'attention_mask': torch.Size([8, 84])}\n",
      "tensor(3.6485, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset['valid'],\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    inputs = {k: v.shape for k, v in batch.items() if k!='labels'}\n",
    "    labels = batch['labels']\n",
    "    \n",
    "    print(labels)\n",
    "    print(inputs)\n",
    "    outputs = model(**batch)\n",
    "    print(outputs.loss, outputs.logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "training_steps = EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3f6af4816c4548868ff0eb2fd058f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# progress_bar = tqdm(range(training_steps))\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for batch in train_dataloader:\n",
    "#         inputs = {k: v.shape for k, v in batch.items() if k!='labels'}\n",
    "#         labels = batch['labels']\n",
    "        \n",
    "#         outputs = model(**batch)\n",
    "\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = load_metric(\"glue\", \"mrpc\")\n",
    "\n",
    "# model.eval()\n",
    "# for batch in eval_dataloader:\n",
    "#     inputs = {k: v.shape for k, v in batch.items() if k!='labels'}\n",
    "#     labels = batch['labels']\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "\n",
    "#     logits = outputs.logits\n",
    "#     preds = torch.argmax(logits, dim=1)\n",
    "#     metric.add_batch(preds, references=labels)\n",
    "\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buat Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = \"RoBERTa\"\n",
    "# tokenizer.save_pretrained(save_directory=save_dir)\n",
    "# model.save_pretrained(save_directory=save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural-language-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
